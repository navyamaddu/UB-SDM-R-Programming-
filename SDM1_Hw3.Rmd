---
title: "Homework_3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


```{r}
library(leaps)
library(glmnet)
library(DAAG)
library(caret)
library(e1071)
library(class)

```

## Problem 1

The goal is to predict who will be interested in buying a caravan insurance policy by applying the models like OLS, Forwards Selection, Backwards Selection, Lasso regression, and Ridge regression. Let us explore the data before training the models.

```{r cars134}
set.seed(1)
Ins_train<-read.delim("ticdata2000.txt", header = FALSE, sep = "\t", dec = ".")
names(Ins_train)[86]<-"Purchase"
dim(Ins_train)
test<-read.delim("ticeval2000.txt",header=FALSE,sep = "\t", dec = ".")
target<-read.delim("tictgts2000.txt",header=FALSE,sep = "\t", dec = ".")
Ins_test<-data.frame(test,target)
names(Ins_test)[86]<-"Purchase"


```

The Insurance data set contains information about customers. It consists of 86 variables and includes product usage data and socio-demographic data derived from zip area codes. The training set contains over 5822 customers, including the information of whether or not they have a caravan insurance policy. The test set contains 4000 customers of whom only the organizers know if they have a caravan insurance policy.

```{r}
Missing_values = ifelse(any(is.na(Ins_train)),"Insurance data contains missing values","There are no missing values in Insurance data set")
Missing_values
```


Now let us perform Least Squares regression on the Insurance data.
```{r}
lm.fit<-lm(Purchase~.,data=Ins_train)
lm.predict_train<-predict(lm.fit,newdata = Ins_train)
print(paste("Train MSE for linear model:",mean((Ins_train$Purchase - lm.predict_train)^2)))
print(paste("Misclassification Rate for linear model(train):",mean(round(lm.predict_train)!=Ins_train$Purchase)))
```

Linear Regression is applied on the Insurance train data and the estimates are predicted for the train and test data.

```{r}
lm.predict_test<-predict(lm.fit,newdata = Ins_test)
print(paste("Test MSE for linear model:", mean((Ins_test$Purchase - lm.predict_test)^2)))
print(paste("Misclassification Rate for linear model(test):",mean(round(lm.predict_test)!=Ins_test$Purchase)))
```

Linear Regression is performed on the Insurance data and the train error(MSE) is observed as 0.05210 and test error(MSE) 0.0539

The Mis-classification Error Rate for train data is 0.0596(5.9%) and for the test data, it is observed as 0.0597(5.9%).

Train MSE is low compared to Test MSE when OLS is performed on the Insurance data.

Now let us compute the confusion matrix for the Linear regression.

```{r}

conf_ols<- confusionMatrix(as.factor(round(lm.predict_test)), as.factor(Ins_test$Purchase),positive = "1")
conf_ols

```

From the confusion matrix, we can say that the model accuracy is 94 %, since 237 buyers and 2 non- buyers are classified in-correctly.

## Forward Subset selection

Forward subset selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. At each step the variable that gives the greatest additional improvement to the fit is added to the model.

Now let us apply forward subset selection to training samples of Insurance data set.

```{r}
forward_subset<-regsubsets(Purchase~.,data=Ins_train,method = "forward",nvmax=85)
summary_forward<-summary(forward_subset)
```

The summary statistics of forward subset selection shows all the best k-predictor models (here range of k is 1 to 85 ) having smallest RSS and highest R squared values.


```{r}
test.mat=model.matrix(Purchase~.,data=Ins_test)
train.mat=model.matrix(Purchase~.,data=Ins_train)
test.errors=rep(NA,85)
train.errors=rep(NA,85)
for(i in 1:85){
  coefi=coef(forward_subset,id=i)

  test_pred=test.mat[,names(coefi)]%*%coefi
 
  train_pred=train.mat[,names(coefi)]%*%coefi
 
  test.errors[i]=mean((Ins_test$Purchase - test_pred)^2)
  
  train.errors[i]=mean((Ins_test$Purchase - train_pred)^2)
  
}

```

We use validation set approach to find the best model among all the k-predictor models. In validation set approach, the prediction is done on the test data by multiplying the coefficients obtained from fitting regsubsets on the train data with method=forward. 

```{r}
plot(test.errors,ylim=c(0.05,0.07),col="red",type="b",xlab="subset size",ylab="MSE",main="Forward subset selection")
lines(train.errors,col="blue",type="b")  
legend("topright",c("Test","Training"),lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
```

The train error is increased from 0.050 to 0.058 while the test error is decreasing gradually from 0.050

```{r}
print(paste("Test MSE for forward subset selection:",min(test.errors)))
print(paste("Train MSE for forward subset selection:",min(train.errors)))

```



```{r}
forward_best_predictors<-which.min(test.errors)  
coef(forward_subset,forward_best_predictors)
```

The minimum test error (0.05385) is obtained for the model with 27 predictors using forward subset selection.

Now let us find the mis-classification error rate for the significant model (27 predictor model)


```{r}
min_bic_forward<-which.min(summary_forward$bic) #8
max_adjr2_forward<-which.max(summary_forward$adjr2) #47
min_cp_forward<-which.min(summary_forward$cp) #23
```

The single best model among k predictor models can also be selected using Cp, BIC, or adjusted R2.

The model with minimum bic or minimum Cp or maximum Adjusted R-squared can be considered as the best model.


```{r}
par(mfrow=c(1,3))
plot(summary_forward$adjr2 ,xlab="Number of Variables ",ylab="Adjusted RSq",type="l")
points(max_adjr2_forward,summary_forward$adjr2[max_adjr2_forward],col="red",pch=20)

plot(summary_forward$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
points(min_bic_forward,summary_forward$bic[min_bic_forward],col="red",pch=20)

plot(summary_forward$cp ,xlab="Number of Variables ",ylab="Cp",type="l")
points(min_cp_forward,summary_forward$cp[min_cp_forward],col="red",pch=20)
```

From the above plots, we can say that 47 predictor model has maximum Adjusted r_squared and 8 predictor and 23 predictor models have best Cp and BIC respectively.

I consider the 27 predictor model obtained from validation set approach to be best since the models obtained from the metrics BIC, Cp, adjusted R-squared are less accurate compared to validation set approach.

```{r}
coefi1=coef(forward_subset,id=27)
test_pred1=test.mat[,names(coefi)]%*%coefi
test.errors1=mean(round(test_pred1)!=Ins_test$Purchase)
print(paste("Mis-classification error rate(Forward):",test.errors1))
```

The mis-classification error rate for the 27 predictor model is observed as 0.05975.

Now let us compute the confusion matrix for the best subset of predictors obtained from forward subset selection.

```{r}
conf_forward<- confusionMatrix(as.factor(round(test_pred1)), as.factor(Ins_test$Purchase),positive = "1")
conf_forward
```

From the confusion matrix, we can observe that the model correctly predicted that 3760 customers did not purchase the insurance whereas the model predicted that only 1 customer purchased the insurance.

## Backward subset selection

Backward stepwise selection  begins with the full least squares model containing all p predictors, and then removes the least useful predictor, one-at-a-time.

Now let us apply backward subset selection to training samples of Insurance data set

```{r}
backward_subset<-regsubsets(Purchase~.,data=Ins_train,method = "backward",nvmax=85)
summary_backward<-summary(backward_subset)
```

The summary statistics of forward subset selection shows all the best k-predictor models(here range of k is 1 to 85 ) having smallest RSS and highest R squared values.

```{r}
test.mat=model.matrix(Purchase~.,data=Ins_test)
train.mat=model.matrix(Purchase~.,data=Ins_train)
test.errors_backward=rep(NA,85)
train.errors_backward=rep(NA,85)
for(i in 1:85){
  coefi=coef(backward_subset,id=i)

  test_pred=test.mat[,names(coefi)]%*%coefi
 
  train_pred=train.mat[,names(coefi)]%*%coefi
 
  test.errors_backward[i]=mean((Ins_test$Purchase - test_pred)^2)
  
  train.errors_backward[i]=mean((Ins_test$Purchase - train_pred)^2)
  
}

```

We use validation set approach to find the best model among all the k-predictor models. In validation set approach, the prediction is done on the test data by multiplying the coefficients obtained from fitting regsubsets on the train data using method=backward


```{r}
plot(test.errors,ylim=c(0.05,0.07),col="red",type="b",xlab="subset size",ylab="MSE",main="Backward subset selection")
lines(train.errors,col="blue",type="b")  
legend("topright",c("Test","Training"),lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
```

Both the train and test MSE are in the range 0.053 to 0.058. 

```{r}
print(paste("Test MSE for backward subset selection:",min(test.errors)))
print(paste("Train MSE for backward subset selection:",min(train.errors)))
```

```{r}
backward_best_predictors<-which.min(test.errors_backward)  
backward_best_predictors

```

The minimum test error (0.05383) is obtained for the model with 38 predictors using forward subset selection.

Now let us find the mis-classification error rate for the significant model (38 predictor model).

```{r}
coefi2=coef(backward_subset,id=38)
test_pred2=test.mat[,names(coefi)]%*%coefi
test.errors2=mean(round(test_pred2)!=Ins_test$Purchase)
print(paste("Mis-classification error rate(Backward):",test.errors2))
```

The mis-classification error rate for the significant model (38 predictor model) using backward subset selection is observed as 0.05975(5.9%)

Let us compute the confusion matrix for best 38 predictor model obtained from backward subset selection.

```{r}
conf_backward<- confusionMatrix(as.factor(round(test_pred2)), as.factor(Ins_test$Purchase),positive = "1")
conf_backward
```

From the confusion matrix, we can say that the model correctly predicted that 3760 customers did not purchase the insurance whereas the model predicted that only 1 customer purchased the insurance out of 238 buyers.

## Ridge Regression

```{r}
Ins_train_x<-model.matrix(Purchase~.,data=Ins_train)[,-1]
Ins_train_y<-Ins_train[,"Purchase"]

Ins_test_x<-model.matrix(Purchase~.,data=Ins_test)[,-1]
Ins_test_y<-Ins_test[,"Purchase"]
```

To perform ridge regression and lasso, we first create x matrix with all the predictors for train and test and a y vector for train and test with the response variable "Purchase".

```{r}
set.seed(131)
ridge.fit<-cv.glmnet(Ins_train_x,Ins_train_y,alpha=0)
```

Now we perform ridge regression on the training matrix x and training vector y with aplha=0 (for ridge). We choose the value of lambda using built in cross-validation function cv.glmnet().

```{r}
bestlam <- ridge.fit$lambda.min
print(paste("Best lambda for ridge:",bestlam))
```

The best value of lambda that results in smallest cross validation error is 0.11182 

We now perform prediction on the test data using the lambda obtained from cross-validation.

```{r}
ridge.pred <- predict(ridge.fit, s = bestlam, newx = Ins_test_x, type = "response")
print(paste("Mis-classification Error rate(test) for ridge regression:",mean(round(ridge.pred)!=Ins_test$Purchase)))
print(paste("Test MSE for ridge regression:",mean((ridge.pred-Ins_test_y)^2)))

```

None of the coefficients obtained from ridge regression are zero indicating that the ridge regression does not perform variable selection.

Let us observe the confusion matrix for the predictors obtained from ridge regression.

```{r}
conf_ridge<- confusionMatrix(as.factor(round(ridge.pred)), as.factor(Ins_test$Purchase),positive = "1")
conf_ridge
```

From the confusion matrix, we can say that the model correctly predicted that 3762 customers did not purchase the insurance whereas the model predicted that only 1 customer purchased the insurance.

## Lasso

```{r}
set.seed(131)
lasso.fit<-cv.glmnet(Ins_train_x,Ins_train_y,alpha=1)
```

Now we perform lasso regression on the training matrix x and training vector y with aplha=1 (for lasso). We choose the value of lambda using built in cross-validation function cv.glmnet().

```{r}
bestlam <- lasso.fit$lambda.min
print(paste("Best lambda for Lasso:",bestlam))
```

The best value of lambda that results in smallest cross validation error is 0.00318.

We now perform prediction on the test data using the lambda obtained from cross-validation.

```{r}
lasso.pred <- predict(lasso.fit, s = bestlam, newx = Ins_test_x, type = "response")
print(paste("Mis-classification error rate for lasso regression:",mean(round(lasso.pred)!=Ins_test$Purchase)))
print(paste("Test MSE for lasso regression:",mean((lasso.pred-Ins_test_y)^2)))

```

The Mean Squared Error for the test data is 0.053760 and the mis-classification error rate is 0.0597(5.9%)

The coefficients of few predictors are zero, indicating that lasso preformed feature selection.

Let us observe the confusion matrix for the predictors obtained from lasso regression.

```{r}

conf_lasso<- confusionMatrix(as.factor(round(lasso.pred)), as.factor(Ins_test$Purchase),positive = "1")
conf_lasso

```

From the confusion matrix, we can say that the model predicted all the 3760 non-buyers correctly but only predicted 1 out 238 buyers correctly.

## Comparison of all the models

```{r}
Model_names<-c("OLS","Forward Subset","Backward Subset","Ridge","Lasso")
Test_MSE<-c(0.053985,0.053855,0.053839,0.053696,0.05376)
Mis_Classification_rate<-c(0.05975,0.05975,0.05975,0.05925,0.05975)
Model_Accuracy<-c("94.02%","94.02%","94.02%","94.08%","94.02%")
sensitivity<-c(0.004,0.004,0.004,0.004,0.004)
specificity<-c(0.99,0.99,0.99,1.00,0.99)
d = list(Model_names,Test_MSE,Mis_Classification_rate,Model_Accuracy,sensitivity,specificity)
Comparison_table <- as.data.frame(d,col.names  = c("Algorithm", "Test MSE","Error rate(test)","Accuracy","Sensitivity","Specificity")) 
Comparison_table
```

From the comparison table, we can observe that Test MSE and Mis-classification error rate are slightly low while the accuracy and sensitivity are a bit high for Ridge Regression compared to all the other models. Hence, we can say that Ridge Regression performed well on the Insurance data set.


```{r}
nrow(Ins_train[Ins_train$Purchase==1,])
```

We cannot predict which customers will be interested in buying the caravan insurance policy.
Although the metrics like Test MSE or Mis-classification error rate are low, the models could not predict most buyers because the training data contains only 348 buyers which constitute only 6% of the data. The reason for low error is that the models could predict all the non-buyers correctly which constitute 94% of the data.

We can clearly say that this data is imbalanced and we do not have enough buyers for the models to learn about the buyer patterns. Since, the data is imbalanced, we observe the phenomenon of "Masking" i.e., one class never dominates and it is mislabeled or mis-assigned to a different class. 

For imbalanced data sets, mis-classification rate/classification accuracy are not good metrics to evaluate a model. Hence, we can use the metrics like Sensitivity (Recall), Precision to evaluate the model performance.

```{r}
conf_ridge
```

From the confusion matrix of the Ridge regression model (low test error among all the models), we can observe that sensitivity (Recall) is 0.004 which indicates the number of buyers predicted correctly over all the true buyers (i.e.,out of 238 actual buyers , only 1 customer is predicted correctly  1/238 = 0.004). Hence, we can say that the model is not good at predicting the buyers.

Positive predicted value (Precision) is observed as 1.0 which indicates that out of all the buyers that are predicted correctly, how many are actual buyers. (1 customer is predicted as buyer and since he is the actual buyer, the precision is observed as 100%).

Specificity (True Negative rate) is observed as 1.00 i.e., number of non-buyers predicted correctly over all the non-buyers (which means out of 3762 non buyers, all are predicted correctly as non-buyers  3762/3762 = 1).

## Problem 2

The goal is to generate a data set with p = 20 features, n = 1000 observations, and an associated quantitative response vector generated according to the model Y= X.beta + noise.

```{r cars}

set.seed(100)
X=matrix(rnorm(1000*20),ncol=20)
beta_1=rnorm(15)
beta_2<-c(0,0,0,0,0)
beta_comb<-c(beta_1,beta_2)
beta<-sample(beta_comb,20,replace = FALSE)
noise=rnorm(20)
Y=X %*% beta + noise

```

Now we have a matrix (X) and a vector Y using rnorm() with few values of beta set to 0's.

Now let us combine the predictors and the corresponding response variables into a data frame and divide the data into train and test in the ratio 80:20.

```{r}
data<-data.frame(X,Y)
indices<-sample(1:length(data$Y),0.8*length(data$Y), replace=FALSE)
train_data<-data[indices,]
test_data<-data[-indices,]
```

Now we have our test and train data ready. Let us perform forward subset selection on the data set.

```{r}
forward_subset<-regsubsets(Y~.,data=train_data,nvmax=20,method = "forward")
summary(forward_subset)$outmat
summary_forward<-summary(forward_subset)
```

The summary statistics of forward subset selection shows all the best k-predictor models(here range of k is 1 to 20) having smallest RSS and highest R squared values.

```{r}
summary_forward$rss
plot(summary_forward$rss,xlab="Subset size",ylab="RSS",type="b")
```

Residual Sum of Squares is high (~9800) for one variable model and gradually reduced to (~712) for 10-variable model and the RSS flattened out after 14-variable model.

```{r}
summary_forward$rsq
plot(summary_forward$rsq,xlab="Subset size",ylab="R-Squared",type="b")
```

We can observe that R-squared statistic increases from 36% when only one variable is included in the model to 96% when 11 variables are included in the model and is almost constant furthur.

```{r}
test.mat=model.matrix(Y~.,data=test_data)
train.mat=model.matrix(Y~.,data=train_data)
test.errors=rep(NA,20)
train.errors=rep(NA,20)
for(i in 1:20){
  coefi=coef(forward_subset,id=i)

  test_pred=test.mat[,names(coefi)]%*%coefi
 
  train_pred=train.mat[,names(coefi)]%*%coefi
 
  test.errors[i]=mean((test_data$Y-test_pred)^2)
  
  train.errors[i]=mean((train_data$Y-train_pred)^2)
  
}

```

We use validation set approach to find the best model among all the k-predictor models. In validation set approach, the prediction is done on the test data by multiplying the coefficients obtained from fitting regsubsets on the train data with method=forward.

```{r}
plot(test.errors,ylim=c(0,20),col="red",type="b",xlab="subset size",ylab="MSE",main="Forward subset selection")
lines(train.errors,col="blue",type="b")  
legend("topright",c("Test","Training"),lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue")) 
```

Test and train errors are high(~14) for one predictor model and are decreasing as the number of predictors are increasing.

The test and train errors almost became constant after the 13 predictor model.

```{r}
print(paste("Test MSE for forward subset selection:",min(test.errors)))
print(paste("Train MSE for forward subset selection:",min(train.errors)))
```

```{r}
forward_best_predictors<-which.min(test.errors)  
coef(forward_subset,forward_best_predictors)
```

The minimum test error (0.71) is obtained for the model with 14 predictors using forward subset selection.

From Validation set approach, we can say that 14 predictor model has minimum test MSE and the significant predictors are X1, X2, X3, X5, X6, X7, X9, X11, X12, X14, X17, X18, X19, X20.


```{r}
beta
```

The generated beta's contain zeros at positions 4, 8, 10, 13, 16 and the coefficents obtained from forward subset selection neglected the predictors X4, X8, X10, X13 and X16.

Now let us use the metrics like bic, cp and adjusted R-squared to find the best predcitor model.

```{r}
min_bic_forward<-which.min(summary_forward$bic) 
max_adjr2_forward<-which.max(summary_forward$adjr2) 
min_cp_forward<-which.min(summary_forward$cp) 
```

The single best model among k predictor models can also be selected using Cp, BIC, or adjusted R2.

The model with minimum bic or minimum Cp or maximum Adjusted R-squared can be considered as the best model

```{r}
par(mfrow=c(1,3))
plot(summary_forward$adjr2 ,xlab="Number of Variables ",ylab="Adjusted RSq",type="l")
points(max_adjr2_forward,summary_forward$adjr2[max_adjr2_forward],col="red",pch=20)

plot(summary_forward$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
points(min_bic_forward,summary_forward$bic[min_bic_forward],col="red",pch=20)

plot(summary_forward$cp ,xlab="Number of Variables ",ylab="Cp",type="l")
points(min_cp_forward,summary_forward$cp[min_cp_forward],col="red",pch=20)
```

From the above plots, we can say that 15 predictor model has maximum Adjusted r_squared and  14 predictor models have best BIC and Cp respectively which is similar to the predictors obtained from validation set approach.

```{r}
which(beta==0)
```

The true model (the generated data) contains 0's for beta values at positions 4, 8, 10, 13 and 16. The best model obtained from forward subset selection neglected the predictors X4, X8, X10, X13 and X16. Hence, we can say that the true model is recovered using forward subset selection.

## Problem 3
# 3a

The goal here is to perform k-nearest neighbors on the iris data set. Let us explore the data and perform exploratory data analysis before training the model.

```{r cars12}
data("iris")
dim(iris)
```

The data set contains 150 rows and 5 columns. Each column gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.

```{r cars1}
summary(iris)

```

From the summary statistics, we can see that the species of Iris have sepal length ranging from 4.3cm to 7.9cm. The mean and median of sepal width is almost same (3.0) indicating that the sepal widths are normally distributed. Petal lengths are in the range 1.0 to 6.9 while petal widths ranges from 0.1 to 2.5. 

```{r pressure}
Missing_values = ifelse(any(is.na(iris)),"Iris data contains missing values","There are no missing values in iris data set")
Missing_values
```

Now let us plot few graphs to visualize the distribution of data

```{r}
par(mfrow=c(2,2))
boxplot(iris$Sepal.Length~iris$Species,ylab="SepalLength(cm)",xlab="Species")
boxplot(iris$Sepal.Width~iris$Species,ylab="Sepal Width(cm)",xlab="Species")
boxplot(iris$Petal.Length~iris$Species,ylab="Petal Length(cm)",xlab="Species")
boxplot(iris$Petal.Width~iris$Species,ylab="Petal Width(cm)",xlab="Species")
```

From the box plots, we can see that Petal length and petal width of virginica are high compared to other two species while the petal length and petal width of setosa speices is very low. 

Now let us divide the data into train and test in the ratio 80:20 to perform k-nearest neighbors.

```{r}
set.seed(13)
indices<-sample(1:length(iris$Sepal.Length),0.8*length(iris$Sepal.Length),replace = FALSE)
iris_train=iris[indices,]
iris_test=iris[-indices,]
iris_train[,1:4]
```

Let us perform k-nearest neighbors for the odd range of k values.

```{r}
k_vals <- c(1,3,5,7,9,11,13,15,17,19,21,23,25,27,29)
store_error_train <- c()
store_error_test <- c()
for (i in seq(1,15)){
  fit_train <- knn(iris_train[,1:4], iris_train[,1:4], iris_train$Species, k = k_vals[i])
  fit_test <- knn(iris_train[,1:4], iris_test[,1:4], iris_train$Species, k = k_vals[i])
  store_error_train[i]<-mean(fit_train!=iris_train$Species)
  store_error_test[i]<-mean(fit_test!=iris_test$Species)
}
#store_error_test

```

Train and test errors are calculated for different k-values and below are the errors.

```{r}
d = list(k_vals,store_error_test,store_error_train)
Error_table <- as.data.frame(d,col.names  = c("k value", "Test Error","Train Error")) 
Error_table
```

The train error is zero indicating that the model is fit perfectly since neighborhoods are sharp with k=1. As the k-value is increasing, train error is fluctuating between 0.03 and 0.05.
The test error is high at k=1 (0.066) indicating that the model is over-fit and then decreased to 0.033 increased to 0.03 (k=5 and k=7) and almost flattened out at zero after k=13.

```{r}
plot(store_error_test,col="red",type="b",xlab="K-value",ylab="Classification Error",main="Test/Train Error for each k",xaxt='n',ylim=c(0,0.07))
axis(1,at = seq(1,15,1),labels = seq(1,29,2),las=1)
lines(store_error_train,col="blue",type="b",xaxt='n') 
legend("topright",c("Test","Training"),lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
```



At low K value (k=1), KNN is trying to fit the to data very closely and trying to find patterns closely. This means, model is going to have probably a good fit for training set due to low bias. Hence, we can see good training error but high test error due to over-fitting since the training time is low.

We can see that the test error is 0.066 initially and then decreased to 0.03 at k=5. Hence, we can consider the best value of k to be 5.

```{r}
fit_test_best_k <- knn(iris_train[,1:4], iris_test[,1:4], iris_train$Species, k = 5)
conf_best_k<-confusionMatrix(factor(fit_test_best_k),iris_test$Species)
conf_best_k
```

From the confusion matrix, we can observe that the model accuracy is 0.96 (96%). All the species of setosa and versicolor are classified correctly while one virginica flower is predicted incorrectly as versicolor. 

## 3b

The goal is to perform k-nearest neighbors on the first two principal components. We perform PCA on the Iris train and test data to obtain first two principal components to evaluate the model performance.


```{r}

fit<-prcomp(iris_train[,1:4],scale=TRUE)
PC_train<-fit$x[,1:2]
df_train<-data.frame(PC_train)
```


```{r}
fit_test<-prcomp(iris_test[,1:4],scale=TRUE)
PC_test<-fit_test$x[,1:2]
df_test<-data.frame(PC_test)
```

Now KNN is applied for a range of k-values on the first two principal components and the Test error is calculated.

```{r}
pca_error <- c()
pca_error_train<-c()
for (i in seq(1,15)){
  fit_test <- knn(df_train, df_test, iris_train$Species, k = k_vals[i])
  fit_train <- knn(df_train, df_train, iris_train$Species, k = k_vals[i])
  pca_error[i]<-mean(fit_test!=iris_test$Species)
  pca_error_train[i]<-mean(fit_train!=iris_train$Species)
}
pca_error
```

Test and train error are plotted for each k value for the first two principal components.

```{r}
plot(pca_error,col="red",type="b",xlab="K-value",ylab="Classification Error",main="Test Error for each k on 2 components",xaxt='n',ylim=c(0,0.3))
lines(pca_error_train,col="blue",type="b",xaxt='n')
axis(1,at = seq(1,15,1),labels = seq(1,29,2),las=1)
legend("topright",c("Test","Training"),lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
```

Now let us compare the test and train errors for different values of k when knn is applied on first 2 principal components.

```{r}
d = list(k_vals,pca_error,pca_error_train)
Error_table_pca <- as.data.frame(d,col.names  = c("k value", "Test Error","Train Error")) 
Error_table_pca
```


The train error is zero at k=1 and gradually increased to 0.091 (k =29) while the test error is 0.100 initially and then increased to 0.20 at k=5 and dropped to 0.133 at k=7.

```{r}
which.min(pca_error)
```


The minimum test error (0.100) is obtained at k =1. Hence, we can consider the best value of k to be 1. 

Now let us compute the confusion matrix for the best k.

```{r}
fit_test_best<- knn(df_train, df_test, iris_train$Species, k = 1)
conf_pca<-confusionMatrix(as.factor(fit_test_best),as.factor(iris_test$Species))
conf_pca
```

From the output of confusion matrix, we can say that the model accuracy is only 90% . We can observe the specificity and positive predicted values of class setosa are 1.0 indicating that the model predicted all the species of setosa accurately. The sensitivity of versicolor and virginica are 86% and 85% respectively since few species are predicted incorrectly.

Now let us plot the scores for the first two principal components on the complete iris data and color the samples by class (Species)

```{r}
fit_all<-prcomp(iris[,1:4],scale=TRUE)
PC<-fit_all$x[,1:2]
df_all<-data.frame(PC)
g <- ggplot(df_all, aes(PC1,PC2)) +geom_point(aes(colour = iris$Species)) + theme(legend.position = "bottom")
plot(g)

```


Now let us plot the scores only for the iris train data.


```{r}
g <- ggplot(df_train, aes(PC1,PC2)) +geom_point(aes(colour = iris_train$Species)) + theme(legend.position = "bottom")
plot(g)

```

Let us plot the Principal component scores for the iris test data colored by species.

```{r}
g <- ggplot(df_test, aes(PC1,PC2)) +geom_point(aes(colour = iris_test$Species)) + theme(legend.position = "bottom")
plot(g)
```


From the plots, we can observe that the principal components of setosa species are well separated from versicolor and virginica and hence the predictions are accurate for the class setosa. 

```{r}
Model_names<-c("knn on full data set","knn on first 2 components")
best_k<-c(5,1)
test_error<-c(0.03,0.100)
accuracy<-c("96%","90%")
d = list(Model_names,best_k,test_error,accuracy)
Comparison_table <- as.data.frame(d,col.names  = c("Model", "Best k-value","Test Error","Accuracy"))
Comparison_table
```

By comparing the part a and b, the test error is low (0.03) when KNN is applied on the complete iris data set compared to test error obtained from applying KNN on the first 2 principal components (0.100).

Model accuracy when KNN is applied on first 2 components is low i.e., the classes are not predicted accurately.

We can observe that best predictions (96% accuracy) are obtained when KNN is applied on iris data set with k=5. 

Setosa and versicolor species are predicted correctly while one virginica flower is predicted incorrectly as versicolor when KNN is applied on the whole data giving an accuracy of 96%.
